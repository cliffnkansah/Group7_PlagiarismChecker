{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **FOR ONLINE CHECKING**"
      ],
      "metadata": {
        "id": "tgd9jg5-7gfs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Package Imports"
      ],
      "metadata": {
        "id": "gEOKDo8L7lzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install docx2txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnHHr3X7_kIX",
        "outputId": "808d715e-b8c9-4283-e8a4-89e381ce3b5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: docx2txt in /usr/local/lib/python3.10/dist-packages (0.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyBnLG-tIuIw",
        "outputId": "d7cc5b1d-d319-4346-ade0-16149dd737f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rECA7zx6InSZ",
        "outputId": "a461e004-aa53-4b2d-c319-9826508c9917"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install fuzzywuzzy"
      ],
      "metadata": {
        "id": "Hxu8VY3YH5lK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tU9HCAMHjiJ"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from fuzzywuzzy import fuzz\n",
        "from difflib import SequenceMatcher\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import difflib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA PREPROCESSING"
      ],
      "metadata": {
        "id": "0rnqUE6Y7xrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "6dK-2wdZH2dJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FUNCTIONS FOR SIMILARITY CALCULATIONS"
      ],
      "metadata": {
        "id": "L_j3eI2qpqwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_similarity(original_text, result_text):\n",
        "    similarity_percentage = fuzz.ratio(original_text, result_text)\n",
        "    return similarity_percentage"
      ],
      "metadata": {
        "id": "SBCYQW_qIUCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_semantic_similarity(original_text, result_text):\n",
        "    # Using SequenceMatcher to get the similarity ratio\n",
        "    seq_matcher = SequenceMatcher(None, original_text, result_text)\n",
        "    similarity_ratio = seq_matcher.ratio()\n",
        "\n",
        "    return similarity_ratio * 100"
      ],
      "metadata": {
        "id": "7zQS8qYVIWwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Acquiring Links from Online Website Checks"
      ],
      "metadata": {
        "id": "dkdlm3jcp6dj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_result_text(link):\n",
        "    try:\n",
        "        response = requests.get(link)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Parse the HTML content using BeautifulSoup\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Extract text content (you might need to adjust this based on the HTML structure)\n",
        "        text_content = ' '.join([p.get_text() for p in soup.find_all('p')])\n",
        "\n",
        "        return text_content\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching content from {link}: {e}\")\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "NVU9dwTmJJlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking Plagiarism Using Api Key\n",
        "\n"
      ],
      "metadata": {
        "id": "9P9QM8PeqB6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def check_plagiarism(query, original_text):\n",
        "    api_key = 'AIzaSyBkFSppbEE56OnNphkDc4db-H6GObiN2BY'\n",
        "    cx = '460b3a177adbd4249'\n",
        "\n",
        "    results = []\n",
        "\n",
        "    try:\n",
        "        preprocessed_query = preprocess_text(query)\n",
        "\n",
        "        url = f\"https://www.googleapis.com/customsearch/v1?q={preprocessed_query}&key={api_key}&cx={cx}\"\n",
        "\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        data = response.json()\n",
        "\n",
        "        # Process the search results\n",
        "        if 'items' in data:\n",
        "            for item in data['items']:\n",
        "                result_text = get_result_text(item['link'])\n",
        "                similarity_percentage = calculate_similarity(original_text, result_text)\n",
        "                semantic_similarity_percentage = calculate_semantic_similarity(original_text, result_text)\n",
        "\n",
        "                result = {\n",
        "                    'link': item['link'],\n",
        "                    'similarity': similarity_percentage,\n",
        "                    'semantic_similarity': semantic_similarity_percentage*100\n",
        "                }\n",
        "\n",
        "                results.append(result)\n",
        "\n",
        "        else:\n",
        "            print(\"No results found.\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error during API request: {e}\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "SeYYh9XaIaEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FOR LOCAL CHECKING**"
      ],
      "metadata": {
        "id": "iabZIlb273OU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Package Imports"
      ],
      "metadata": {
        "id": "pLWVfGBy8HJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import chardet\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "import itertools\n",
        "import docx2txt\n",
        "import nltk\n",
        "from fuzzywuzzy import fuzz\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ],
      "metadata": {
        "id": "v2bVu3Xd7_kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA PROCESSING"
      ],
      "metadata": {
        "id": "6m0KxEe_9DhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = nltk.word_tokenize(text)\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words and word.isalpha()]\n",
        "    return ' '.join(filtered_words)\n"
      ],
      "metadata": {
        "id": "MlkmTb6H8iTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking Similarity Between Texts"
      ],
      "metadata": {
        "id": "RWj2Ag79qQcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_cosine_similarity(text1, text2):\n",
        "    processed_text1 = preprocess(text1)\n",
        "    processed_text2 = preprocess(text2)\n",
        "\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform([processed_text1, processed_text2])\n",
        "\n",
        "    similarity_score = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n",
        "    return similarity_score\n"
      ],
      "metadata": {
        "id": "gDkAifko9KAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting Similarity Scores"
      ],
      "metadata": {
        "id": "QjrRFvwkqfxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_plagiarism_graph(similarity_scores):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.hist(similarity_scores, bins=10, alpha=0.7, color='blue')\n",
        "    plt.title('Plagiarism Rate Distribution')\n",
        "    plt.xlabel('Similarity Score')\n",
        "    plt.ylabel('Frequency')\n",
        "    st.pyplot(plt)\n",
        "\n"
      ],
      "metadata": {
        "id": "VmS1G0fJ9vTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Highlight Plagiarised Texts"
      ],
      "metadata": {
        "id": "fYUejTDPqmxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def highlight_plagiarized(text, reference_text):\n",
        "    # Get the difference between the two texts\n",
        "    d = difflib.Differ()\n",
        "    diff = list(d.compare(text.splitlines(), reference_text.splitlines()))\n",
        "\n",
        "    # Build the highlighted text\n",
        "    highlighted_text = \"\"\n",
        "    for line in diff:\n",
        "        if line.startswith('  '):  # Unchanged\n",
        "            highlighted_text += line[2:] + '\\n'\n",
        "        elif line.startswith('- '):  # Removed\n",
        "            highlighted_text += f'<span style=\"background-color:#000080;\">{line[2:]}</span>\\n'\n",
        "        elif line.startswith('+ '):  # Added\n",
        "            highlighted_text += f'<span style=\"background-color:#000080;\">{line[2:]}</span>\\n'\n",
        "\n",
        "    return highlighted_text"
      ],
      "metadata": {
        "id": "oEbhWp9T_nKT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}