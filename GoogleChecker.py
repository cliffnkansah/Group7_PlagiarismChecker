# -*- coding: utf-8 -*-
"""GooglePlagiarism.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gC5OwCxkQsyDgDee8L0_F7-1EAA__oPb

# **FOR ONLINE CHECKING**

# Package Imports
"""

# !pip install docx2txt

import nltk

nltk.download('stopwords')

nltk.download('punkt')

from difflib import SequenceMatcher

import requests
# !pip install fuzzywuzzy
import streamlit as st
from bs4 import BeautifulSoup
from fuzzywuzzy import fuzz
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
import difflib

"""# DATA PREPROCESSING"""

def preprocess_text(text):
    tokens = word_tokenize(text.lower())
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]
    stemmer = PorterStemmer()
    tokens = [stemmer.stem(word) for word in tokens]
    return ' '.join(tokens)

def calculate_similarity(original_text, result_text):
    similarity_percentage = fuzz.ratio(original_text, result_text)
    return similarity_percentage

def calculate_semantic_similarity(original_text, result_text):
    # Using SequenceMatcher to get the similarity ratio
    seq_matcher = SequenceMatcher(None, original_text, result_text)
    similarity_ratio = seq_matcher.ratio()

    return similarity_ratio * 100

def get_result_text(link):
    try:
        response = requests.get(link)
        response.raise_for_status()

        # Parse the HTML content using BeautifulSoup
        soup = BeautifulSoup(response.text, 'html.parser')

        # Extract text content (you might need to adjust this based on the HTML structure)
        text_content = ' '.join([p.get_text() for p in soup.find_all('p')])

        return text_content

    except requests.exceptions.RequestException as e:
        print(f"Error fetching content from {link}: {e}")
        return ""

def check_plagiarism(query, original_text):
    api_key = 'AIzaSyBkFSppbEE56OnNphkDc4db-H6GObiN2BY'
    cx = '460b3a177adbd4249'

    results = []

    try:
        preprocessed_query = preprocess_text(query)

        url = f"https://www.googleapis.com/customsearch/v1?q={preprocessed_query}&key={api_key}&cx={cx}"

        response = requests.get(url)
        response.raise_for_status()

        data = response.json()

        # Process the search results
        if 'items' in data:
            for item in data['items']:
                result_text = get_result_text(item['link'])
                similarity_percentage = calculate_similarity(original_text, result_text)
                semantic_similarity_percentage = calculate_semantic_similarity(original_text, result_text)

                result = {
                    'link': item['link'],
                    'similarity': similarity_percentage,
                    'semantic_similarity': semantic_similarity_percentage*100
                }

                results.append(result)

        else:
            print("No results found.")

    except requests.exceptions.RequestException as e:
        print(f"Error during API request: {e}")

    return results



"""# **FOR LOCAL CHECKING**

# Package Imports
"""

import itertools
import shutil
import zipfile

import chardet
import docx2txt
import matplotlib.pyplot as plt
import nltk
import pandas as pd
from fuzzywuzzy import fuzz
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

"""# DATA PROCESSING"""

def preprocess(text):
    stop_words = set(stopwords.words('english'))
    words = nltk.word_tokenize(text)
    filtered_words = [word for word in words if word.lower() not in stop_words and word.isalpha()]
    return ' '.join(filtered_words)

def calculate_cosine_similarity(text1, text2):
    processed_text1 = preprocess(text1)
    processed_text2 = preprocess(text2)

    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform([processed_text1, processed_text2])

    similarity_score = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]
    return similarity_score

def plot_plagiarism_graph(similarity_scores):
    plt.figure(figsize=(8, 6))
    plt.hist(similarity_scores, bins=10, alpha=0.7, color='blue')
    plt.title('Plagiarism Rate Distribution')
    plt.xlabel('Similarity Score')
    plt.ylabel('Frequency')
    st.pyplot(plt)




def highlight_plagiarized(text, reference_text):
    # Get the difference between the two texts
    d = difflib.Differ()
    diff = list(d.compare(text.splitlines(), reference_text.splitlines()))

    # Build the highlighted text
    highlighted_text = ""
    for line in diff:
        if line.startswith('  '):  # Unchanged
            highlighted_text += line[2:] + '\n'
        elif line.startswith('- '):  # Removed
            highlighted_text += f'<span style="background-color:#000080;">{line[2:]}</span>\n'
        elif line.startswith('+ '):  # Added
            highlighted_text += f'<span style="background-color:#000080;">{line[2:]}</span>\n'

    return highlighted_text


